# üèóÔ∏è Architecture du Syst√®me

Ce document d√©crit l'architecture de haut niveau de WhytChat, une application de chat locale s√©curis√©e utilisant Tauri, Rust et des mod√®les d'IA locaux.

## üß© Vue d'Ensemble

WhytChat suit une architecture **Monolithique Modulaire** distribu√©e en deux processus principaux (Frontend & Backend) communiquant via IPC.

### Diagramme de Haut Niveau

```mermaid
graph TD
    subgraph "Frontend (Electron-like)"
        UI[React UI] -->|Invoke / Events| IPC[Tauri IPC Bridge]
        Store[Zustand Store] <--> UI
    end

    subgraph "Backend (Rust Core)"
        IPC --> Main[Main Entry Point]
        Main --> AppState[AppState (Global Lock)]

        subgraph "Actor System (Tokio)"
            AppState --> Supervisor[Supervisor Actor]
            Supervisor --> LLM[LLM Actor]
            Supervisor --> RAG[RAG Actor]
            Supervisor --> Brain[Brain Analyzer]
        end

        subgraph "Persistence"
            RAG --> VectorDB[LanceDB (Vectors)]
            Main --> SQLite[SQLite (Chat History)]
            Main --> FS[PortablePathManager (Files)]
        end
    end

    LLM -->|HTTP| LlamaServer[llama-server.exe (GGUF)]
```

---

## üõ†Ô∏è Stack Technique

### Frontend (`apps/desktop-ui`)

- **Framework** : React 18
- **Build Tool** : Vite
- **Styling** : Tailwind CSS
- **State Management** : Zustand (avec persistance)
- **Langue** : JavaScript (ES6+)

### Backend (`apps/core`)

- **Langage** : Rust (Edition 2021)
- **Framework App** : Tauri 2.0 (Beta/RC)
- **Async Runtime** : Tokio
- **Base de Donn√©es** :
  - Relationnelle : `sqlx` (SQLite)
  - Vectorielle : `lancedb` + `fastembed`
- **Architecture** : Actor Model (impl√©mentation custom sur Tokio Channels)

### Intelligence Artificielle

- **Inf√©rence LLM** : `llama-server` (binaire externe pilot√© via HTTP)
- **Mod√®le LLM** : GGUF (ex: Qwen 2.5 7B)
- **Embeddings** : ONNX Runtime via `fastembed` (`AllMiniLML6V2`)
- **Classification** : "The Brain" (Regex + Fallback S√©mantique)

---

## üß† Le Module "Brain"

Le "Brain" est un module d'analyse pr√©-LLM con√ßu pour router les requ√™tes intelligemment sans latence.

```mermaid
graph LR
    Input[User Input] --> Intent{Intent Classification}

    Intent -->|Regex Match| FastPath[Fast Path (~1ms)]
    Intent -->|No Match| Semantic[Semantic Fallback (~50ms)]

    FastPath --> ContextBuilder
    Semantic --> ContextBuilder

    ContextBuilder -->|Context Packet| Supervisor
```

Voir [IA_INTERNALS.md](./IA_INTERNALS.md) pour les d√©tails.

---

## üíæ Gestion des Donn√©es

### Syst√®me de Fichiers (PortablePathManager)

Pour assurer la portabilit√© (notamment sur cl√© USB), aucun chemin absolu n'est utilis√© en dur. Le `fs_manager.rs` r√©sout dynamiquement les chemins :

- `data/` : Base de donn√©es, vecteurs, mod√®les.
- `config/` : Fichiers de configuration.

### Base de Donn√©es (SQLite)

- **Sessions** : Conversations actives.
- **Messages** : Historique des chats.
- **Library_Files** : Registre global des fichiers import√©s.
- **Session_Files_Link** : Table de liaison (Many-to-Many) entre Sessions et Fichiers.

---

## üîí S√©curit√©

- **Chiffrement** : Les configurations sensibles (cl√©s API si existantes, param√®tres syst√®me) sont chiffr√©es au repos (`encryption.rs`) utilisant `Aes256Gcm`.
- **Isolation** : Le LLM tourne dans un processus s√©par√©. Le Frontend n'a pas d'acc√®s direct au disque (passe par le Backend).
- **Contr√¥le d'Acc√®s** : Les fichiers ne sont accessibles au RAG que s'ils sont explicitement li√©s √† la session active.

---

_Derni√®re mise √† jour : Novembre 2025_
