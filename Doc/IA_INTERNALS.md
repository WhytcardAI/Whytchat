# üß† IA Internals : Le Cerveau de WhytChat

Ce document explique le fonctionnement interne des composants d'Intelligence Artificielle de WhytChat.

## ü§ñ Mod√®le de Langage (LLM)

WhytChat n'utilise pas d'API externe (OpenAI, Anthropic). Tout tourne localement.

- **Moteur** : `llama-server` (bas√© sur `llama.cpp`).
- **Format** : GGUF (GPT-Generated Unified Format).
- **Mod√®le par d√©faut** : `Qwen 2.5 7B Instruct` (Quantized Q4_K_M).
  - _Pourquoi ce choix ?_ Excellent support du fran√ßais, tr√®s performant pour le code et capable de suivre des instructions complexes avec une empreinte m√©moire raisonnable (4.6GB).

## üß© Le Module Brain V2

Le "Brain" est la couche d'intelligence interm√©diaire entre l'utilisateur et le LLM. Il a pour but de r√©duire la latence et d'am√©liorer la pertinence des r√©ponses en analysant la requ√™te AVANT de solliciter le mod√®le lourd.

### Architecture Two-Tier

Pour garantir une r√©activit√© maximale, l'analyse se fait en deux temps :

1.  **Fast Path (Regex)** - _~1ms_
    - Utilise des expressions r√©guli√®res compil√©es (Rust `regex`) pour d√©tecter des intentions √©videntes ("Code moi...", "Traduis...", "Bonjour").
    - Si une correspondance est trouv√©e avec une confiance > 0.8, l'analyse s'arr√™te l√†.

2.  **Semantic Fallback (Embeddings)** - _~50ms_
    - Si aucune regex ne matche, la requ√™te est vectoris√©e (voir ci-dessous).
    - Le vecteur est compar√© (Cosine Similarity) avec des vecteurs de r√©f√©rence pour chaque intention (Question, Commande, Cr√©atif, etc.).
    - C'est plus lent mais beaucoup plus robuste aux variations de langage.

### Structure du ContextPacket

Le Brain produit un `ContextPacket` qui est transmis au LLM via le Supervisor :

```rust
pub struct ContextPacket {
    pub intent: IntentResult,           // L'intention d√©tect√©e (ex: CodeRequest)
    pub keywords: Vec<KeywordResult>,   // Mots-cl√©s extraits (TF-IDF)
    pub complexity: ComplexityMetrics,  // Score de complexit√© (0.0 - 1.0)
    pub language: Language,             // Langue d√©tect√©e (FR/EN)
    pub rag_results: Vec<RagResult>,    // Documents pertinents (si RAG activ√©)
}
```

## üìö RAG (Retrieval-Augmented Generation)

Le syst√®me RAG permet au LLM de "lire" vos documents.

### Stack Vectorielle

- **Embeddings** : `fastembed` (Rust wrapper pour ONNX Runtime).
  - Mod√®le : `AllMiniLML6V2` (Transforme le texte en vecteurs de 384 dimensions).
  - _Note_ : Mod√®le tr√®s l√©ger (~23MB) et rapide, optimis√© pour le CPU.
- **Stockage** : `LanceDB` (Base de donn√©es vectorielle embarqu√©e, sans serveur).
  - Stocke les vecteurs et les m√©tadonn√©es sur le disque local.

### Workflow d'Ingestion

1.  Fichier upload√© -> Texte brut.
2.  D√©coupage (Chunking) : Par ligne (split `\n`) avec filtre de longueur minimale.
3.  Vectorisation : `Text -> [0.12, -0.45, ...]`
4.  Indexation : LanceDB √©crit les donn√©es sur le disque avec le tag `file:{id}`.

### Workflow de Recherche

1.  Question utilisateur -> Vecteur Query.
2.  Recherche ANN (Approximate Nearest Neighbor) dans LanceDB.
3.  **Filtrage** : Restriction aux fichiers li√©s √† la session (`metadata IN ('file:ID1', ...)`).
4.  R√©cup√©ration des 3 chunks les plus proches (Top-K).
5.  Injection dans le prompt syst√®me :
    > "Context:\n[CHUNK 1]\n[CHUNK 2]..."

## üîÆ √âvolutions Futures (Phase 2)

- Remplacement des Regex par un mod√®le ONNX d√©di√© (`DistilBERT`) pour la classification d'intention.
- Ajout de NER (Named Entity Recognition) pour extraire des noms propres et lieux.
- Am√©lioration du Chunking (Recursive Character Splitter) pour ne pas couper les phrases.

---

_Bas√© sur ARCHITECTURE_BRAIN_V2.md - Novembre 2025_
