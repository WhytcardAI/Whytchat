# ğŸ”„ Flux de DonnÃ©es - WhytChat V1

> Documentation complÃ¨te des flux de donnÃ©es dans l'application

---

## ğŸ“‘ Table des MatiÃ¨res

1. [Flux Principal : Message Chat](#1-flux-principal--message-chat)
2. [Flux Secondaires](#2-flux-secondaires)
3. [Diagrammes de SÃ©quence](#3-diagrammes-de-sÃ©quence)

---

## 1. Flux Principal : Message Chat

### Vue d'Ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FLUX D'UN MESSAGE CHAT                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

USER INPUT (ChatInput.jsx)
    â”‚
    â–¼ onSend(text)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChatInterface.jsx                                                          â”‚
â”‚   - CrÃ©e session si null (createSession)                                   â”‚
â”‚   - Appelle sendMessage(text, sessionId)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼ useChatStream.sendMessage()
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ useChatStream.js                                                           â”‚
â”‚   1. Ajoute user msg localement (setMessages)                              â”‚
â”‚   2. setThinking(true), clearThinkingSteps()                               â”‚
â”‚   3. invoke('debug_chat', {session_id, message})                           â”‚
â”‚   4. Ã‰coute Ã©vÃ©nements 'chat-token' et 'thinking-step'                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼ Tauri IPC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ main.rs :: debug_chat                                                      â”‚
â”‚   1. check_rate_limit_and_get_resources()                                  â”‚
â”‚   2. supervisor.process_message(session_id, message, Some(window))         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼ SupervisorHandle.process_message()
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ supervisor.rs                                                              â”‚
â”‚   1. Sauvegarde user msg en DB (database::add_message)                     â”‚
â”‚   2. Brain Analysis (BrainAnalyzer::analyze)                               â”‚
â”‚      â””â”€ Intent, Keywords, Complexity, Language, RAG decision               â”‚
â”‚   3. emit("thinking-step") + emit("brain-analysis")                        â”‚
â”‚   4. SI should_use_rag â†’ RAG search (rag.search_with_filters)              â”‚
â”‚   5. Build ChatML prompt avec contexte                                     â”‚
â”‚   6. LLM streaming (llm.stream_generate_with_params)                       â”‚
â”‚      â””â”€ Pour chaque token: emit("chat-token")                              â”‚
â”‚   7. Sauvegarde assistant msg en DB                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Ã‰tape 1 : Input Utilisateur

**Fichier** : `ChatInput.jsx`

```jsx
const handleSubmit = () => {
  if (!text.trim()) return;
  onSend(text.trim());  // â†’ ChatInterface.handleSend()
  setText('');
};
```

---

### Ã‰tape 2 : PrÃ©paration Frontend

**Fichier** : `useChatStream.js`

```javascript
const sendMessage = async (text, sessionId) => {
  // 1. Optimistic update
  setMessages((prev) => [...prev, {
    id: crypto.randomUUID(),
    role: 'user',
    content: text,
    created_at: new Date().toISOString(),
  }]);

  // 2. Setup thinking state
  setThinking(true);
  clearThinkingSteps();

  // 3. Setup response handlers
  let assistantContent = '';
  messageHandler = (payload) => {
    assistantContent += payload.content;
    updateAssistantMessage(assistantContent);
  };
  
  thinkingHandler = (payload) => {
    addThinkingStep(payload);
  };

  // 4. Invoke backend
  await invoke('debug_chat', {
    session_id: sessionId,
    message: text,
  });
};
```

---

### Ã‰tape 3 : Commande Tauri

**Fichier** : `main.rs`

```rust
#[tauri::command]
async fn debug_chat(
    session_id: String,
    message: String,
    window: Window,
    state: State<'_, AppState>,
) -> Result<String, String> {
    // 1. Rate limiting
    let (supervisor, _pool) = check_rate_limit_and_get_resources(&state)
        .await
        .map_err(|e| e.to_string())?;

    // 2. Process message
    supervisor
        .process_message(session_id, message, Some(window))
        .await
        .map_err(|e| e.to_string())
}
```

---

### Ã‰tape 4 : Orchestration Supervisor

**Fichier** : `supervisor.rs`

```rust
async fn handle_process_message(
    &mut self,
    session_id: String,
    message: String,
    window: Option<Window>,
) -> Result<String, AppError> {
    // 1. Save user message
    database::add_message(&self.pool, &session_id, "user", &message).await?;

    // 2. Brain analysis
    let context = self.brain.analyze(&message).await;
    emit_thinking(&window, "Analyzing query...", &format!("{:?}", context));
    emit_brain_analysis(&window, &context);

    // 3. RAG search (conditional)
    let rag_context = if context.should_use_rag {
        emit_thinking(&window, "Searching knowledge base...", "");
        let results = self.rag.search(&message, 5).await?;
        format_rag_results(&results)
    } else {
        String::new()
    };

    // 4. Build prompt
    let prompt = build_chatml_prompt(&message, &rag_context, &context);

    // 5. Stream LLM response
    emit_thinking(&window, "Generating response...", "");
    let response = self.stream_llm(&prompt, &window).await?;

    // 6. Save assistant message
    database::add_message(&self.pool, &session_id, "assistant", &response).await?;

    Ok(response)
}
```

---

### Ã‰tape 5 : Brain Analysis

**Fichier** : `brain/analyzer.rs`

```rust
pub async fn analyze(&self, query: &str) -> ContextPacket {
    // 1. Intent classification (fast path)
    let (intent, confidence) = self.intent_classifier.classify(query);
    
    // 2. Semantic fallback if low confidence
    let final_intent = if confidence < 0.5 {
        self.semantic_intent.classify(query).await
    } else {
        intent
    };

    // 3. Keyword extraction
    let keywords = self.keyword_extractor.extract(query);

    // 4. Complexity scoring
    let complexity = self.complexity_scorer.score(query);

    // 5. Language detection
    let language = detect_language(query);

    // 6. RAG decision
    let should_use_rag = self.should_use_rag(&final_intent, &keywords);

    ContextPacket {
        intent: final_intent,
        confidence,
        keywords,
        complexity,
        language,
        should_use_rag,
        suggested_strategies: vec![],
    }
}
```

---

### Ã‰tape 6 : RAG Search

**Fichier** : `actors/rag.rs`

```rust
pub async fn search(
    &self,
    query: &str,
    top_k: usize,
) -> Result<Vec<SearchResult>, AppError> {
    // 1. Generate query embedding
    let query_vec = self.embed(query).await?;

    // 2. Vector search
    let results = self.table
        .search(&query_vec)
        .limit(top_k)
        .execute()
        .await?;

    // 3. Convert to SearchResult
    let search_results = results
        .iter()
        .map(|r| SearchResult {
            content: r.content.clone(),
            metadata: serde_json::from_str(&r.metadata).ok(),
            score: r.score,
        })
        .collect();

    Ok(search_results)
}
```

---

### Ã‰tape 7 : LLM Streaming

**Fichier** : `actors/llm.rs`

```rust
pub async fn stream_generate(
    &self,
    prompt: &str,
    window: &Option<Window>,
) -> Result<String, AppError> {
    // 1. Ensure llama-server is running
    self.ensure_server_running().await?;

    // 2. HTTP request with streaming
    let response = self.client
        .post("http://localhost:8080/completion")
        .header("Authorization", format!("Bearer {}", self.auth_token))
        .json(&json!({
            "prompt": prompt,
            "stream": true,
            "temperature": 0.7,
            "max_tokens": 4096,
        }))
        .send()
        .await?;

    // 3. Parse SSE stream
    let mut full_response = String::new();
    let mut stream = response.bytes_stream();

    while let Some(chunk) = stream.next().await {
        let chunk = chunk?;
        let text = String::from_utf8_lossy(&chunk);
        
        for line in text.lines() {
            if line.starts_with("data: ") {
                let data = &line[6..];
                if let Ok(json) = serde_json::from_str::<Value>(data) {
                    if let Some(content) = json["content"].as_str() {
                        full_response.push_str(content);
                        
                        // Emit token to frontend
                        if let Some(w) = window {
                            w.emit("chat-token", json!({ "content": content }))?;
                        }
                    }
                }
            }
        }
    }

    Ok(full_response)
}
```

---

## 2. Flux Secondaires

### 2.1 CrÃ©ation de Session

```
Frontend                          Backend
   â”‚                                 â”‚
   â”‚ invoke('create_session', {      â”‚
   â”‚   title, language,              â”‚
   â”‚   system_prompt, temperature    â”‚
   â”‚ })                              â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
   â”‚                                 â”‚ 1. Validate inputs
   â”‚                                 â”‚ 2. Generate UUID
   â”‚                                 â”‚ 3. Encrypt model_config
   â”‚                                 â”‚ 4. INSERT INTO sessions
   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ session_id                      â”‚
```

### 2.2 Upload de Fichier

```
Frontend                          Backend
   â”‚                                 â”‚
   â”‚ invoke('upload_file_for_session', {
   â”‚   session_id, file_path         â”‚
   â”‚ })                              â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
   â”‚                                 â”‚ 1. Copy file to data/files/
   â”‚                                 â”‚ 2. Extract text (PDF/DOCX)
   â”‚                                 â”‚ 3. Generate embeddings
   â”‚                                 â”‚ 4. Store in LanceDB
   â”‚                                 â”‚ 5. INSERT INTO library_files
   â”‚                                 â”‚ 6. INSERT INTO session_files
   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ file_id                         â”‚
```

### 2.3 Recherche RAG Manuelle

```
Frontend                          Backend
   â”‚                                 â”‚
   â”‚ invoke('search_knowledge', {    â”‚
   â”‚   query, top_k                  â”‚
   â”‚ })                              â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
   â”‚                                 â”‚ 1. Generate query embedding
   â”‚                                 â”‚ 2. Vector search LanceDB
   â”‚                                 â”‚ 3. Format results
   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Vec<SearchResult>               â”‚
```

---

## 3. Diagrammes de SÃ©quence

### 3.1 Initialisation Application

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ App.jsx â”‚ â”‚main.rs  â”‚ â”‚preflight â”‚ â”‚ DB  â”‚ â”‚ RAG â”‚ â”‚  LLM   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚           â”‚            â”‚          â”‚       â”‚        â”‚
     â”‚ mount     â”‚            â”‚          â”‚       â”‚        â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚            â”‚          â”‚       â”‚        â”‚
     â”‚           â”‚            â”‚          â”‚       â”‚        â”‚
     â”‚           â”‚ preflight_check()     â”‚       â”‚        â”‚
     â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚          â”‚       â”‚        â”‚
     â”‚           â”‚            â”‚ check_dirs        â”‚        â”‚
     â”‚           â”‚            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚       â”‚        â”‚
     â”‚           â”‚            â”‚ check_model       â”‚        â”‚
     â”‚           â”‚            â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”‚        â”‚
     â”‚           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”‚       â”‚        â”‚
     â”‚           â”‚            â”‚          â”‚       â”‚        â”‚
     â”‚           â”‚ initialize_app()      â”‚       â”‚        â”‚
     â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â–ºâ”‚        â”‚
     â”‚           â”‚                       â”‚       â”‚ init   â”‚
     â”‚           â”‚                       â”‚       â”œâ”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚           â”‚                       â”‚       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚           â”‚                       â”‚â—„â”€â”€â”€â”€â”€â”€â”¤        â”‚
     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                       â”‚       â”‚        â”‚
     â”‚ ready     â”‚                       â”‚       â”‚        â”‚
```

### 3.2 Message Chat Complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”
â”‚ChatInput â”‚â”‚useChatSt.â”‚â”‚ main.rs â”‚â”‚Supervisorâ”‚â”‚Brainâ”‚â”‚ RAG â”‚â”‚ LLM â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â””â”€â”€â”¬â”€â”€â”˜â””â”€â”€â”¬â”€â”€â”˜â””â”€â”€â”¬â”€â”€â”˜
     â”‚           â”‚           â”‚          â”‚         â”‚      â”‚      â”‚
     â”‚ submit    â”‚           â”‚          â”‚         â”‚      â”‚      â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚           â”‚          â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚           â”‚          â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚ invoke('debug_chat') â”‚         â”‚      â”‚      â”‚
     â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚          â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚           â”‚          â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚           â”‚ process_message()  â”‚      â”‚      â”‚
     â”‚           â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚         â”‚      â”‚      â”‚
     â”‚           â”‚           â”‚          â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚           â”‚          â”‚ analyze()       â”‚      â”‚
     â”‚           â”‚           â”‚          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚      â”‚      â”‚
     â”‚           â”‚           â”‚          â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚      â”‚
     â”‚           â”‚           â”‚          â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚ emit('thinking-step')â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚      â”‚      â”‚
     â”‚           â”‚           â”‚          â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚           â”‚          â”‚ search()â”‚      â”‚      â”‚
     â”‚           â”‚           â”‚          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â–ºâ”‚      â”‚
     â”‚           â”‚           â”‚          â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤      â”‚
     â”‚           â”‚           â”‚          â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚           â”‚          â”‚ stream_generate()     â”‚
     â”‚           â”‚           â”‚          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚           â”‚           â”‚          â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚ emit('chat-token') â—„â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
     â”‚           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚      â”‚      â”‚
     â”‚           â”‚           â”‚          â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚ [repeat per token]   â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚           â”‚          â”‚         â”‚      â”‚      â”‚
     â”‚           â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ done    â”‚      â”‚      â”‚
     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”‚          â”‚         â”‚      â”‚      â”‚
     â”‚ display   â”‚           â”‚          â”‚         â”‚      â”‚      â”‚
```

---

## ğŸ“Š RÃ©capitulatif des Ã‰vÃ©nements

| Ã‰vÃ©nement | Direction | Payload | Quand |
|-----------|-----------|---------|-------|
| `chat-token` | Backend â†’ Frontend | `{ content: string }` | Chaque token LLM |
| `thinking-step` | Backend â†’ Frontend | `{ step: string, details: string }` | Chaque Ã©tape |
| `brain-analysis` | Backend â†’ Frontend | `ContextPacket` | AprÃ¨s analyse Brain |

---

## ğŸ“š Voir Aussi

- [02_ARCHITECTURE.md](02_ARCHITECTURE.md) - Architecture globale
- [03_BACKEND_RUST.md](03_BACKEND_RUST.md) - Backend Rust
- [04_FRONTEND_REACT.md](04_FRONTEND_REACT.md) - Frontend React

---

_Document gÃ©nÃ©rÃ© le 27 novembre 2025_
