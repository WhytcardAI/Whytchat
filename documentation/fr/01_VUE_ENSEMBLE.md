# ğŸ”­ Vue d'Ensemble - WhytChat V1

> Application de chat IA locale avec RAG, construite avec Tauri 2.0

---

## ğŸ“‹ Informations Projet

| Champ       | Valeur                        |
| ----------- | ----------------------------- |
| Nom         | whytchat-core                 |
| Version     | 1.0.0                         |
| Identifiant | com.whytcard.whytchat-v1      |
| Rust        | 1.80.0+ (rust-toolchain.toml) |
| Tauri       | 2.0.0-rc                      |

---

## ğŸ—ï¸ Structure Monorepo

```
WhytChat_V1/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ core/                    # Backend Rust (Tauri)
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.rs          # Point d'entrÃ©e, 22 commandes Tauri
â”‚   â”‚   â”‚   â”œâ”€â”€ actors/          # SystÃ¨me d'acteurs (LLM, RAG, Supervisor)
â”‚   â”‚   â”‚   â”œâ”€â”€ brain/           # Analyse prÃ©-LLM (intent, keywords, complexity)
â”‚   â”‚   â”‚   â”œâ”€â”€ database.rs      # CRUD SQLite
â”‚   â”‚   â”‚   â”œâ”€â”€ encryption.rs    # AES-256-GCM
â”‚   â”‚   â”‚   â”œâ”€â”€ error.rs         # AppError centralisÃ©
â”‚   â”‚   â”‚   â”œâ”€â”€ fs_manager.rs    # PortablePathManager
â”‚   â”‚   â”‚   â”œâ”€â”€ models.rs        # Structs (Session, Message, etc.)
â”‚   â”‚   â”‚   â”œâ”€â”€ rate_limiter.rs  # Limite 20 req/min par session
â”‚   â”‚   â”‚   â””â”€â”€ text_extract.rs  # PDF, DOCX, TXT, CSV, JSON
â”‚   â”‚   â”œâ”€â”€ migrations/          # SchÃ©ma SQLite
â”‚   â”‚   â””â”€â”€ Cargo.toml
â”‚   â”‚
â”‚   â””â”€â”€ desktop-ui/              # Frontend React
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ App.jsx          # Point d'entrÃ©e
â”‚       â”‚   â”œâ”€â”€ components/      # Composants React
â”‚       â”‚   â”œâ”€â”€ hooks/           # useChatStream, etc.
â”‚       â”‚   â”œâ”€â”€ store/           # Zustand (appStore.js)
â”‚       â”‚   â””â”€â”€ lib/             # Utilitaires
â”‚       â””â”€â”€ package.json
â”‚
â”œâ”€â”€ data/                        # DonnÃ©es runtime
â”‚   â”œâ”€â”€ db/                      # whytchat.sqlite
â”‚   â”œâ”€â”€ files/                   # Fichiers uploadÃ©s
â”‚   â”œâ”€â”€ models/                  # ModÃ¨les GGUF + embeddings
â”‚   â””â”€â”€ vectors/                 # LanceDB (knowledge_base.lance)
â”‚
â””â”€â”€ package.json                 # Monorepo root
```

---

## ğŸ”§ Stack Technique

### Backend (apps/core)

| DÃ©pendance  | Version  | RÃ´le                       |
| ----------- | -------- | -------------------------- |
| tauri       | 2.0.0-rc | Framework desktop          |
| sqlx        | 0.8      | SQLite async               |
| lancedb     | 0.10     | Base vectorielle           |
| fastembed   | 4        | Embeddings AllMiniLML6V2   |
| aes-gcm     | 0.10.3   | Encryption configurations  |
| reqwest     | 0.12     | HTTP client (llama-server) |
| tokio       | 1        | Runtime async              |
| tracing     | 0.1      | Logging structurÃ©          |
| pdf-extract | 0.7      | Extraction PDF             |
| docx-rs     | 0.4      | Extraction DOCX            |

### Frontend (apps/desktop-ui)

| DÃ©pendance      | Version | RÃ´le                 |
| --------------- | ------- | -------------------- |
| react           | 18.3.1  | UI Framework         |
| vite            | 5.4.1   | Build tool           |
| zustand         | 5.0.0   | State management     |
| tailwindcss     | 3.4.10  | CSS utility          |
| @tauri-apps/api | 2.0.0   | Bridge Tauri         |
| i18next         | 25.6.3  | Internationalisation |
| lucide-react    | 0.454.0 | Icons                |
| react-hot-toast | 2.6.0   | Notifications        |

---

## ğŸ“Š Constantes du Projet

```rust
// main.rs
const DEFAULT_MODEL_FILENAME: &str = "default-model.gguf";
const LLAMA_SERVER_URL: &str = "https://github.com/ggml-org/llama.cpp/releases/download/b4154/llama-b4154-bin-win-avx2-x64.zip";
const MODEL_URL: &str = "https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q4_k_m.gguf";
const MIN_MODEL_SIZE_BYTES: u64 = 3 * 1024 * 1024 * 1024; // 3 GB minimum
```

---

## ğŸš€ ModÃ¨le LLM

| PropriÃ©tÃ©    | Valeur                                   |
| ------------ | ---------------------------------------- |
| ModÃ¨le       | Qwen2.5-Coder-7B-Instruct                |
| Quantization | Q4_K_M                                   |
| Taille       | ~4.7 GB                                  |
| Format       | GGUF                                     |
| Contexte     | 8192 tokens                              |
| Serveur      | llama-server (llama.cpp b4154)           |
| Port         | 8080                                     |
| Template     | ChatML (`<\|im_start\|>...<\|im_end\|>`) |

---

## ğŸ” SÃ©curitÃ©

- **Encryption** : AES-256-GCM pour `model_config` des sessions
- **Nonce** : AlÃ©atoire 12 bytes par encryption (rand::thread_rng)
- **ClÃ©** : 32 bytes stockÃ©e dans `data/.encryption_key`
- **Auth** : `LLAMA_AUTH_TOKEN` gÃ©nÃ©rÃ© au dÃ©marrage (UUID)
- **Rate Limit** : 20 requÃªtes/minute par session

---

## ğŸ“ Tests

```bash
# 44 tests unitaires Rust
cargo test --manifest-path apps/core/Cargo.toml

# Tests passÃ©s:
# - brain::* (14 tests) - Intent, keywords, complexity
# - encryption::* (1 test)
# - rate_limiter::* (2 tests)
# - text_extract::* (8 tests)
```

---

_GÃ©nÃ©rÃ© depuis lecture directe de: main.rs, Cargo.toml, package.json, tauri.conf.json_
