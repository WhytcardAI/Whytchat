# üóÇÔ∏è Architecture Interactive WhytChat - Pr√©sentation Technique

## üéØ Vue d'ensemble

WhytChat est une application de chat IA locale et priv√©e construite avec **Tauri + Rust + React**. Cette pr√©sentation explore l'architecture technique √† travers des diagrammes interactifs Mermaid.

---

## üîÑ 1. Traitement Complet d'une Requ√™te Chat

### Description

Ce diagramme montre le flux complet d'une requ√™te utilisateur dans le syst√®me WhytChat, de l'interface React jusqu'√† la g√©n√©ration de r√©ponse avec streaming temps r√©el.

### Points Cl√©s

- ‚úÖ **Sessions persist√©es** : Chaque conversation a sa propre configuration
- ‚úÖ **Configuration dynamique** : `system_prompt` et `temperature` par session
- ‚úÖ **Multi-agent orchestration** : Analyse ‚Üí RAG ‚Üí G√©n√©ration
- ‚úÖ **Streaming temps r√©el** : Tokens envoy√©s au frontend via Tauri events
- ‚úÖ **Persistance compl√®te** : Historique et messages sauvegard√©s

### Diagramme de S√©quence - Traitement Chat

```mermaid
sequenceDiagram
    actor User as "User"
    participant UI as "React ChatInterface"
    participant Store as "Zustand useAppStore"
    participant Tauri as "Tauri Command 'processUserMessage'"
    participant SupHandle as "SupervisorHandle"
    participant SupRunner as "SupervisorRunner"
    participant DB as "Database (SQLite)"
    participant LLM as "LlmActorHandle / LlmActorRunner"
    participant RAG as "RagActorHandle"

    User->>UI: "Type message and press send"
    UI->>Store: "get currentSessionId"
    UI->>Tauri: "invoke('process_user_message', { session_id, content })"

    Tauri->>SupHandle: "send SupervisorMessage::ProcessUserMessage { session_id, content }"
    SupHandle->>SupRunner: "forward message via mpsc channel"

    SupRunner->>DB: "get_session(session_id)"
    DB-->>SupRunner: "Session with ModelConfig { system_prompt, temperature }"

    SupRunner->>Tauri: "emit thinking event: 'Analyse de la demande...'"

    SupRunner->>LLM: "generate_with_params(analysis_prompt, system_prompt, temperature)"
    LLM->>LLM: "build JSON payload with 'prompt', optional 'system_prompt', optional 'temperature'"
    LLM->>"llama-server": "HTTP POST '/completion' (stream: false)"
    "llama-server"-->>LLM: "JSON { content }"
    LLM-->>SupRunner: "analysis String"

    SupRunner->>Tauri: "emit thinking step: 'Analyse effectu√©e' (example)"

    SupRunner->>RAG: "search_with_session(content, session_id)"
    RAG-->>SupRunner: "context snippets"

    SupRunner->>SupRunner: "compose final_prompt with user content, analysis, RAG context"

    SupRunner->>Tauri: "emit thinking event: 'G√©n√©ration de la r√©ponse...'"

    SupRunner->>LLM: "stream_generate_with_params(final_prompt, system_prompt, temperature, chunk_sender)"
    LLM->>"llama-server": "HTTP POST '/completion' (stream: true)"
    loop "SSE stream of tokens"
        "llama-server"-->>LLM: "SSE line 'data: {json}'"
        LLM->>LLM: "parse JSON, extract 'content' token"
        LLM-->>SupRunner: "send Ok(token) via chunk_sender"
        SupRunner-->>Tauri: "emit streaming token event"
        Tauri-->>UI: "frontend event with token"
        UI->>UI: "append token to assistant message"
    end

    SupRunner->>DB: "persist user and assistant messages for session_id"
    DB-->>SupRunner: "OK"
    SupRunner-->>Tauri: "final result (completion)"
    Tauri-->>UI: "resolve invoke promise"
    UI-->>User: "Display full assistant reply"
```

### Explication D√©taill√©e

1. **Trigger UI** : L'utilisateur tape un message et appuie sur "Envoyer"
2. **State Management** : R√©cup√©ration de l'ID de session courante depuis Zustand
3. **Tauri Bridge** : Invocation de la commande Rust `process_user_message`
4. **Supervisor** : Routage vers le SupervisorHandle qui forward via channel
5. **Configuration Session** : R√©cup√©ration de la `ModelConfig` depuis SQLite
6. **Analyse** : G√©n√©ration d'une analyse courte avec les param√®tres de session
7. **RAG Search** : Recherche documentaire dans les fichiers de la session
8. **Composition** : Construction du prompt final avec contexte
9. **G√©n√©ration** : G√©n√©ration streaming avec param√®tres de session
10. **Persistance** : Sauvegarde des messages en base

---

## ‚ûï 2. Cr√©ation d'une Nouvelle Session

### Description

Processus de cr√©ation d'une nouvelle session de chat avec configuration par d√©faut et persistance en base de donn√©es.

### Configuration par D√©faut

```json
ModelConfig {
  "model_id": "qwen2.5-7b-instruct-q4_k_m.gguf",
  "temperature": 0.7,
  "system_prompt": "You are a helpful AI assistant."
}
```

### Diagramme de S√©quence - Cr√©ation Session

```mermaid
sequenceDiagram
    actor User as "User"
    participant MainLayout as "React MainLayout"
    participant Store as "Zustand useAppStore"
    participant Tauri as "Tauri Command 'create_session'"
    participant DB as "Database (SQLite)"

    User->>MainLayout: "Click 'New Chat' button"
    MainLayout->>Store: "createSession()"

    Store->>Tauri: "invoke('create_session')"
    Tauri->>DB: "create_session(pool, title = 'Nouvelle session', ModelConfig {...})"
    DB-->>Tauri: "Session { id, title, model_config, ... }"
    Tauri-->>Store: "sessionId String"

    Store->>Store: "append { id: sessionId, created_at: now } to sessions[]"
    Store->>Store: "set currentSessionId = sessionId"

    Store-->>MainLayout: "updated sessions and currentSessionId state"
    MainLayout->>MainLayout: "sidebar lists new session as active"

    note over Store,Tauri: "Other components (e.g., ChatInterface) can also call<br/>createSession() and loadSessions()/get_session_messages via Tauri."
```

### √âtapes du Processus

1. **Trigger UI** : Clic sur "New Chat" dans la sidebar
2. **Store Action** : Appel de `createSession()` dans le store Zustand
3. **Tauri Command** : Invocation de la commande Rust `create_session`
4. **Database** : Cr√©ation de la session avec configuration par d√©faut
5. **State Update** : Mise √† jour du state avec la nouvelle session
6. **UI Update** : Sidebar affiche la nouvelle session comme active

### Points d'Extension

- üîß **Configuration personnalisable** : Interface pour modifier system_prompt et temperature
- üìù **Titres dynamiques** : G√©n√©ration automatique bas√©e sur le premier message
- üè∑Ô∏è **Cat√©gorisation** : Tags et cat√©gories pour organiser les sessions
- üîÑ **Templates** : Sessions pr√©-configur√©es pour diff√©rents cas d'usage

---

## üèóÔ∏è 3. Architecture des Acteurs

### Description

Relations et structure des acteurs du syst√®me multi-agent : Supervisor, LLM et RAG.

### Avantages de l'Architecture

- üîÑ **Isolation** : Chaque actor fonctionne ind√©pendamment
- üìà **Extensibilit√©** : Nouveaux actors faciles √† ajouter
- üõ°Ô∏è **R√©silience** : Un actor qui crash n'affecte pas les autres
- ‚ö° **Performance** : Traitement asynchrone et parall√©lisation
- üîß **Maintenabilit√©** : Code modulaire et testable

### Diagramme de Classes - Acteurs

```mermaid
classDiagram
    class SupervisorHandle {
        +sender: mpsc::Sender~SupervisorMessage~
        +new() SupervisorHandle
        +new_with_pool(db_pool: Option~SqlitePool~) SupervisorHandle
        +new_with_pool_and_model(db_pool: Option~SqlitePool~, model_path: PathBuf) SupervisorHandle
        +process_user_message(...) -> Result~(), ActorError~
        +ingest_content(content: String, metadata: Option~String~) -> Result~(), ActorError~
    }

    class SupervisorRunner {
        -receiver: mpsc::Receiver~SupervisorMessage~
        -llm_actor: LlmActorHandle
        -rag_actor: RagActorHandle
        -db_pool: Option~SqlitePool~
        +new(receiver: mpsc::Receiver~SupervisorMessage~, db_pool: Option~SqlitePool~, model_path: PathBuf) SupervisorRunner
        +run() async
        -handle_message(msg: SupervisorMessage) async
    }

    class LlmActorHandle {
        +sender: mpsc::Sender~LlmMessage~
        +new(model_path: PathBuf) LlmActorHandle
        +generate(prompt: String) -> Result~String, ActorError~
        +generate_with_params(prompt: String, system_prompt: Option~String~, temperature: Option~f32~) -> Result~String, ActorError~
        +stream_generate(prompt: String, chunk_sender: mpsc::Sender~Result~String, ActorError~~) -> Result~(), ActorError~
        +stream_generate_with_params(prompt: String, system_prompt: Option~String~, temperature: Option~f32~, chunk_sender: mpsc::Sender~Result~String, ActorError~~) -> Result~(), ActorError~
    }

    class LlmActorRunner {
        -receiver: mpsc::Receiver~LlmMessage~
        -child: Option~tokio::process::Child~
        -server_url: String
        -model_path: PathBuf
        -client: Client
        +new(receiver: mpsc::Receiver~LlmMessage~, model_path: PathBuf) LlmActorRunner
        +run() async
        -start_server() async -> Result~(), ActorError~
        -handle_message(msg: LlmMessage) async
        -generate_completion(prompt: String, system_prompt: Option~String~, temperature: Option~f32~) async -> Result~String, ActorError~
        -stream_completion(prompt: String, system_prompt: Option~String~, temperature: Option~f32~, chunk_sender: mpsc::Sender~Result~String, ActorError~~) async -> Result~(), ActorError~
    }

    class RagActorHandle {
        +sender: mpsc::Sender~RagMessage~
        +new() RagActorHandle
        +new_with_options(vectors_path: Option~PathBuf~, db_pool: Option~SqlitePool~) RagActorHandle
        +ingest(content: String, metadata: Option~String~) -> Result~String, ActorError~
        +search(query: String, session_id: Option~String~, limit: usize) -> Result~Vec~String~, ActorError~
        +search_with_session(query: String, session_id: Option~String~) -> Result~Vec~String~, ActorError~
    }

    class ActorError {
        +LlmError(String)
        +RagError(String)
        +Internal(String)
    }

    class LlmMessage {
        +Generate { prompt: String, system_prompt: Option~String~, temperature: Option~f32~, responder: oneshot::Sender~Result~String, ActorError~~ }
        +StreamGenerate { prompt: String, system_prompt: Option~String~, temperature: Option~f32~, chunk_sender: mpsc::Sender~Result~String, ActorError~~, responder: oneshot::Sender~Result~(), ActorError~~ }
    }

    class SupervisorMessage {
        +ProcessUserMessage { session_id: String, content: String, window: Option~Window~, responder: oneshot::Sender~Result~String, ActorError~~ }
        +IngestContent { content: String, metadata: Option~String~, responder: oneshot::Sender~Result~String, ActorError~~ }
        +Shutdown
    }

    SupervisorHandle --> SupervisorRunner : "spawns and controls"
    SupervisorRunner --> LlmActorHandle : "holds"
    SupervisorRunner --> RagActorHandle : "holds"
    LlmActorHandle --> LlmActorRunner : "spawns and controls"

    SupervisorRunner ..> SupervisorMessage : "uses"
    SupervisorRunner ..> LlmMessage : "sends"
    RagActorHandle ..> ActorError : "returns"
    LlmActorHandle ..> ActorError : "returns"
    LlmActorRunner ..> ActorError : "returns"
    SupervisorRunner ..> ActorError : "returns"
```

### D√©tail des Acteurs

#### üé≠ **Supervisor Actor**

**R√¥le** : Orchestrateur principal du syst√®me multi-agent

```
M√©thodes cl√©s :
‚Ä¢ process_user_message() - Traite les messages utilisateur
‚Ä¢ ingest_content() - Ingest des documents
‚Ä¢ handle_message() - Route les messages internes
```

**Relations** : Contr√¥le LLM et RAG actors

#### ü§ñ **LLM Actor**

**R√¥le** : Interface avec le mod√®le Llama.cpp local

```
M√©thodes cl√©s :
‚Ä¢ generate_with_params() - G√©n√©ration simple
‚Ä¢ stream_generate_with_params() - G√©n√©ration streaming
‚Ä¢ start_server() - Lance llama-server
```

**Configuration** : `system_prompt`, `temperature` par requ√™te

#### üìö **RAG Actor**

**R√¥le** : Recherche documentaire avec embeddings

```
M√©thodes cl√©s :
‚Ä¢ ingest() - Indexe les documents
‚Ä¢ search_with_session() - Recherche contextuelle
‚Ä¢ search() - Recherche g√©n√©rale
```

**Technologies** : FastEmbed + LanceDB

#### üí¨ **Messages**

**R√¥le** : Protocole de communication asynchrone

```
Types de messages :
‚Ä¢ LlmMessage - Requ√™tes LLM
‚Ä¢ RagMessage - Requ√™tes RAG
‚Ä¢ SupervisorMessage - Coordination
```

**Pattern** : Channel-based avec oneshot responders

---

## üõ†Ô∏è Technologies & Architecture

### Stack Technique

- **Backend** : Rust + Tauri + Acteurs Tokio
- **Frontend** : React + TypeScript + Zustand
- **IA** : Llama.cpp + Qwen 2.5 7B + RAG
- **Base** : SQLite + FastEmbed + LanceDB
- **Communication** : Tauri commands + Events

### Patterns Architecturaux

- **Handle/Runner Pattern** : S√©paration interface/publique vs logique interne
- **Actor Model** : Communication asynchrone via channels
- **Session-based** : Isolation des contextes par conversation
- **Streaming** : G√©n√©ration temps r√©el avec feedback UI

### S√©curit√© & Performance

- **Local-first** : Aucune donn√©e ne quitte la machine
- **Streaming optimis√©** : Tokens envoy√©s d√®s g√©n√©ration
- **RAG contextuel** : Recherche limit√©e √† la session active
- **Configuration persist√©e** : Param√®tres sauvegard√©s par session

---

## üöÄ Points d'Extension

### Configuration Avanc√©e

- Interface pour personnaliser `system_prompt` et `temperature` par session
- Templates de sessions pr√©-configur√©es
- Gestion des mod√®les multiples

### Am√©liorations IA

- Fine-tuning des mod√®les
- Multi-modalit√© (images, audio)
- Cha√Ænage d'agents plus complexe

### UX/UI

- Mode sombre/clair
- Th√®mes personnalisables
- Raccourcis clavier √©tendus

### Performance

- Cache intelligent des embeddings
- Optimisation m√©moire des mod√®les
- Parall√©lisation des recherches RAG

---

## üìä M√©triques & Monitoring

### M√©triques Cl√©s

- **Latence** : Temps de premi√®re r√©ponse
- **Throughput** : Tokens/seconde en streaming
- **Pr√©cision RAG** : Pertinence des r√©sultats de recherche
- **Utilisation m√©moire** : Impact des mod√®les charg√©s

### Logging

- Logs structur√©s par actor
- Tra√ßabilit√© des sessions
- M√©triques de performance

---

_Pr√©sentation g√©n√©r√©e automatiquement - Architecture WhytChat v1.0.0_
